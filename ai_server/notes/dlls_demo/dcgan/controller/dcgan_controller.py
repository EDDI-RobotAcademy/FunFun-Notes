from fastapi import APIRouter, File, UploadFile, HTTPException, status
from fastapi.responses import JSONResponse
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import torchvision.datasets as datasets
from torch.utils.data import DataLoader
from io import BytesIO
from PIL import Image
import os

# FastAPI ë¼ìš°í„° ìƒì„±
dcganRouter = APIRouter()

# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
latent_dim = 100
epochs = 10
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Generator ì •ì˜
class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        self.model = nn.Sequential(
            nn.ConvTranspose2d(latent_dim, 512, 4, 1, 0, bias=False),
            nn.BatchNorm2d(512),
            nn.ReLU(True),

            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(True),

            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(True),

            nn.ConvTranspose2d(128, 3, 4, 2, 1, bias=False),  # RGB ì±„ë„ë¡œ ìˆ˜ì •
            nn.Tanh()
        )

    def forward(self, x):
        return self.model(x)

# Discriminator ì •ì˜
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.model = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),
            nn.LeakyReLU(0.2),
            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),
            nn.LeakyReLU(0.2),
            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),
            nn.LeakyReLU(0.2),
            nn.Conv2d(256, 1, kernel_size=4, stride=1, padding=0),
            nn.AdaptiveAvgPool2d((1, 1)),  # ğŸ”¥ ì¶”ê°€
            nn.Sigmoid()
        )

    def forward(self, x):
        output = self.model(x)
        # print(f"Discriminator output shape: {output.shape}")  # ì¶œë ¥ í¬ê¸° í™•ì¸
        return output.view(x.size(0), -1)  # (batch_size, 1)ë¡œ ê°•ì œ ë³€í™˜

# ëª¨ë¸ ì´ˆê¸°í™”
G = Generator().to(device)
D = Discriminator().to(device)

# í•™ìŠµ API
@dcganRouter.post("/dcgan/train")
async def train_dcgan(batch_size: int = 128):
    print(f"[INFO] Training DCGAN with batch size {batch_size} on {device}")

    transform = transforms.Compose([
        transforms.Resize((64, 64)),  # 64x64ë¡œ í¬ê¸° ì¡°ì •
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # RGB ì´ë¯¸ì§€ë¥¼ [-1, 1] ë²”ìœ„ë¡œ ì •ê·œí™”
    ])

    # CIFAR-10 ë°ì´í„°ì…‹
    dataset = datasets.CIFAR10(root="./data", train=True, download=True, transform=transform)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    criterion = nn.BCELoss()
    optimizer_G = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))
    optimizer_D = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))

    for epoch in range(epochs):
        for i, (real_imgs, _) in enumerate(dataloader):
            real_imgs = real_imgs.to(device)
            batch_size = real_imgs.size(0)

            real_labels = torch.ones(batch_size, 1).to(device)
            fake_labels = torch.zeros(batch_size, 1).to(device)

            optimizer_D.zero_grad()

            real_output = D(real_imgs)
            # print(f"real_output shape: {real_output.shape}, real_labels shape: {real_labels.shape}")  # ë””ë²„ê¹… ì¶œë ¥
            real_loss = criterion(real_output, real_labels)  # ì—¬ê¸°ì„œ ì—ëŸ¬ ë°œìƒ ê°€ëŠ¥ì„± ìˆìŒ

            z = torch.randn(batch_size, latent_dim, 1, 1).to(device)
            fake_imgs = G(z).detach()
            fake_output = D(fake_imgs)
            fake_loss = criterion(fake_output, fake_labels)

            d_loss = real_loss + fake_loss
            d_loss.backward()
            optimizer_D.step()

            # Generator í•™ìŠµ
            optimizer_G.zero_grad()
            fake_output = D(G(z))
            g_loss = criterion(fake_output, real_labels)
            g_loss.backward()
            optimizer_G.step()

            if i % 100 == 0:
                print(f"Epoch [{epoch + 1}/{epochs}] | Step [{i}/{len(dataloader)}] | "
                      f"D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}")

    os.makedirs("models", exist_ok=True)
    torch.save(G.state_dict(), "models/dcgan_generator.pth")
    print("[INFO] Training completed! Model saved at models/dcgan_generator.pth")
    return {"message": "âœ… DCGAN í•™ìŠµ ì™„ë£Œ! ëª¨ë¸ ì €ì¥ë¨."}


# ì´ë¯¸ì§€ ìƒì„± API
@dcganRouter.post("/dcgan/generate")
async def generate_image():
    if not os.path.exists("models/dcgan_generator.pth"):
        raise HTTPException(status_code=404, detail="Model not found. Please train the model first.")

    print("[INFO] Loading trained generator model...")
    G.load_state_dict(torch.load("models/dcgan_generator.pth", map_location=device), strict=False)
    G.eval()

    z = torch.randn(1, latent_dim, 1, 1).to(device)
    with torch.no_grad():
        generated_image = G(z).cpu().squeeze(0).permute(1, 2, 0)

    transform_to_pil = transforms.ToPILImage()
    generated_pil_image = transform_to_pil(generated_image)

    img_bytes = BytesIO()
    generated_pil_image.save(img_bytes, format="PNG")
    img_bytes.seek(0)

    print("[INFO] Image generation successful!")
    return JSONResponse(content={"message": "Image generated successfully"}, status_code=status.HTTP_200_OK,
                        media_type="image/png",
                        headers={"Content-Disposition": "attachment; filename=generated_image.png",
                                 "Content-Type": "image/png"})
